{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PM2.5 Hexagon Data Enrichment\n",
    "\n",
    "This notebook creates a unified dataset by:\n",
    "1. Identifying all PM2.5 measurement hexagons at H3 resolution 7\n",
    "2. Enriching them with traffic and weather data (local or nearest-neighbor)\n",
    "3. Adding static features like terrain elevation\n",
    "4. Creating a query interface for location-based lookups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 1: Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries loaded successfully\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import h3\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "from tqdm import tqdm\n",
    "import folium\n",
    "from folium import plugins\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "\n",
    "print(\"Libraries loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data path: /Users/vojtech/Code/Bard89/Project-Data/data/processed/\n",
      "H3 Resolution: 7 (approx 5.16 km² per hexagon)\n"
     ]
    }
   ],
   "source": [
    "# Define data paths\n",
    "DATA_PATH = '/Users/vojtech/Code/Bard89/Project-Data/data/processed/'\n",
    "\n",
    "# H3 resolution for analysis\n",
    "H3_RESOLUTION = 7  # 5.16 km² hexagons\n",
    "\n",
    "print(f\"Data path: {DATA_PATH}\")\n",
    "print(f\"H3 Resolution: {H3_RESOLUTION} (approx 5.16 km² per hexagon)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 2: Create PM2.5 Hexagon Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading PM2.5 air quality data...\n",
      "Loaded 9,579,181 PM2.5 records\n",
      "Date range: 2023-07-14 16:00:00+00:00 to 2025-07-26 05:00:00+00:00\n",
      "PM2.5 missing values: 20.0%\n"
     ]
    }
   ],
   "source": [
    "# Load PM2.5 data\n",
    "print(\"Loading PM2.5 air quality data...\")\n",
    "df_pm25 = pd.read_csv(f\"{DATA_PATH}jp_openaq_processed_20230101_to_20231231.csv\",\n",
    "                       usecols=['h3_index_res8', 'h3_lat_res8', 'h3_lon_res8', \n",
    "                                'timestamp', 'pm25_ugm3_mean'])\n",
    "\n",
    "print(f\"Loaded {len(df_pm25):,} PM2.5 records\")\n",
    "print(f\"Date range: {df_pm25['timestamp'].min()} to {df_pm25['timestamp'].max()}\")\n",
    "print(f\"PM2.5 missing values: {df_pm25['pm25_ugm3_mean'].isna().mean():.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating PM2.5 hexagon registry at H3 resolution 7...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 643/643 [00:00<00:00, 67847.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created registry with 634 PM2.5 hexagons at resolution 7\n",
      "\n",
      "Top 5 hexagons by measurement count:\n",
      "             hex7_id  center_lat  center_lon  measurement_count\n",
      "466  872f5bc81ffffff      35.675     139.440              30977\n",
      "427  872f5aaccffffff      35.607     139.722              30835\n",
      "193  872e61ae1ffffff      34.401     135.304              22810\n",
      "464  872f5bc24ffffff      35.465     139.470              18604\n",
      "595  874b65d09ffffff      33.958     131.948              15910\n"
     ]
    }
   ],
   "source": [
    "# Create PM2.5 hexagon registry at resolution 7\n",
    "print(\"\\nCreating PM2.5 hexagon registry at H3 resolution 7...\")\n",
    "\n",
    "pm25_registry = {}\n",
    "\n",
    "for _, row in tqdm(df_pm25[['h3_index_res8', 'h3_lat_res8', 'h3_lon_res8']].drop_duplicates().iterrows(), \n",
    "                   total=df_pm25['h3_index_res8'].nunique()):\n",
    "    if pd.notna(row['h3_index_res8']):\n",
    "        hex7 = h3.cell_to_parent(row['h3_index_res8'], H3_RESOLUTION)\n",
    "        \n",
    "        if hex7 not in pm25_registry:\n",
    "            pm25_registry[hex7] = {\n",
    "                'hex7_id': hex7,\n",
    "                'center_lat': row['h3_lat_res8'],\n",
    "                'center_lon': row['h3_lon_res8'],\n",
    "                'res8_hexagons': [],\n",
    "                'measurement_count': 0\n",
    "            }\n",
    "        \n",
    "        pm25_registry[hex7]['res8_hexagons'].append(row['h3_index_res8'])\n",
    "\n",
    "# Calculate measurement counts\n",
    "hex7_counts = df_pm25.groupby(\n",
    "    df_pm25['h3_index_res8'].apply(lambda x: h3.cell_to_parent(x, H3_RESOLUTION) if pd.notna(x) else None)\n",
    ")['pm25_ugm3_mean'].count()\n",
    "\n",
    "for hex7, count in hex7_counts.items():\n",
    "    if hex7 in pm25_registry:\n",
    "        pm25_registry[hex7]['measurement_count'] = count\n",
    "\n",
    "print(f\"\\nCreated registry with {len(pm25_registry)} PM2.5 hexagons at resolution 7\")\n",
    "\n",
    "# Convert to DataFrame\n",
    "pm25_hex_df = pd.DataFrame(pm25_registry.values())\n",
    "pm25_hex_df = pm25_hex_df.sort_values('measurement_count', ascending=False)\n",
    "\n",
    "print(\"\\nTop 5 hexagons by measurement count:\")\n",
    "print(pm25_hex_df[['hex7_id', 'center_lat', 'center_lon', 'measurement_count']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 3: Load and Map Auxiliary Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading traffic data...\n",
      "Found 1017 traffic hexagons at resolution 7\n",
      "PM2.5 hexagons with local traffic data: 26 (4.1%)\n"
     ]
    }
   ],
   "source": [
    "# Load traffic data\n",
    "print(\"Loading traffic data...\")\n",
    "df_traffic = pd.read_csv(f\"{DATA_PATH}jp_jartic_processed_20230101_to_20231231.csv\",\n",
    "                          usecols=['h3_index_res8', 'h3_lat_res8', 'h3_lon_res8',\n",
    "                                   'timestamp', 'avg_traffic_volume'])\n",
    "\n",
    "# Create traffic hexagon registry at resolution 7\n",
    "traffic_registry = {}\n",
    "for _, row in df_traffic[['h3_index_res8', 'h3_lat_res8', 'h3_lon_res8']].drop_duplicates().iterrows():\n",
    "    if pd.notna(row['h3_index_res8']):\n",
    "        hex7 = h3.cell_to_parent(row['h3_index_res8'], H3_RESOLUTION)\n",
    "        if hex7 not in traffic_registry:\n",
    "            traffic_registry[hex7] = {\n",
    "                'hex7_id': hex7,\n",
    "                'center_lat': row['h3_lat_res8'],\n",
    "                'center_lon': row['h3_lon_res8']\n",
    "            }\n",
    "\n",
    "print(f\"Found {len(traffic_registry)} traffic hexagons at resolution 7\")\n",
    "\n",
    "# Check overlap with PM2.5\n",
    "pm25_with_traffic = set(pm25_registry.keys()) & set(traffic_registry.keys())\n",
    "print(f\"PM2.5 hexagons with local traffic data: {len(pm25_with_traffic)} ({len(pm25_with_traffic)/len(pm25_registry)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weather data...\n",
      "Found 536 weather hexagons at resolution 7\n",
      "PM2.5 hexagons with local weather data: 8 (1.3%)\n"
     ]
    }
   ],
   "source": [
    "# Load weather data\n",
    "print(\"Loading weather data...\")\n",
    "df_weather = pd.read_csv(f\"{DATA_PATH}jp_openmeteo_processed_20230101_to_20231231.csv\",\n",
    "                          usecols=['h3_index_res8', 'h3_lat_res8', 'h3_lon_res8',\n",
    "                                   'timestamp', 'temperature_c_mean', 'humidity_pct_mean',\n",
    "                                   'precipitation_mm_mean'])\n",
    "\n",
    "# Create weather hexagon registry at resolution 7\n",
    "weather_registry = {}\n",
    "for _, row in df_weather[['h3_index_res8', 'h3_lat_res8', 'h3_lon_res8']].drop_duplicates().iterrows():\n",
    "    if pd.notna(row['h3_index_res8']):\n",
    "        hex7 = h3.cell_to_parent(row['h3_index_res8'], H3_RESOLUTION)\n",
    "        if hex7 not in weather_registry:\n",
    "            weather_registry[hex7] = {\n",
    "                'hex7_id': hex7,\n",
    "                'center_lat': row['h3_lat_res8'],\n",
    "                'center_lon': row['h3_lon_res8']\n",
    "            }\n",
    "\n",
    "print(f\"Found {len(weather_registry)} weather hexagons at resolution 7\")\n",
    "\n",
    "# Check overlap with PM2.5\n",
    "pm25_with_weather = set(pm25_registry.keys()) & set(weather_registry.keys())\n",
    "print(f\"PM2.5 hexagons with local weather data: {len(pm25_with_weather)} ({len(pm25_with_weather)/len(pm25_registry)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 4: Build Nearest Neighbor Lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def haversine_distance(lat1, lon1, lat2, lon2):\n",
    "    \"\"\"Calculate the great circle distance between two points on Earth\"\"\"\n",
    "    from math import radians, cos, sin, asin, sqrt\n",
    "    \n",
    "    # Convert to radians\n",
    "    lat1, lon1, lat2, lon2 = map(radians, [lat1, lon1, lat2, lon2])\n",
    "    \n",
    "    # Haversine formula\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2\n",
    "    c = 2 * asin(sqrt(a))\n",
    "    r = 6371  # Radius of Earth in kilometers\n",
    "    \n",
    "    return c * r\n",
    "\n",
    "def find_nearest_hexagon(target_hex, target_lat, target_lon, source_registry, max_distance_km=500):\n",
    "    \"\"\"Find nearest hexagon from source registry\"\"\"\n",
    "    min_distance = float('inf')\n",
    "    nearest_hex = None\n",
    "    \n",
    "    for source_hex, source_info in source_registry.items():\n",
    "        if source_hex == target_hex:\n",
    "            # Same hexagon - distance is 0\n",
    "            return source_hex, 0.0\n",
    "        \n",
    "        distance = haversine_distance(target_lat, target_lon, \n",
    "                                       source_info['center_lat'], \n",
    "                                       source_info['center_lon'])\n",
    "        \n",
    "        if distance < min_distance and distance < max_distance_km:\n",
    "            min_distance = distance\n",
    "            nearest_hex = source_hex\n",
    "    \n",
    "    return nearest_hex, min_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building nearest neighbor lookup table...\n",
      "This may take a few minutes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PM2.5 hexagons: 100%|██████████| 634/634 [00:00<00:00, 1323.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Nearest neighbor statistics:\n",
      "Hexagons with local traffic: 26\n",
      "Hexagons with local weather: 8\n",
      "\n",
      "Traffic distance statistics (km):\n",
      "count    608.000\n",
      "unique   608.000\n",
      "top        3.899\n",
      "freq       1.000\n",
      "Name: traffic_distance_km, dtype: float64\n",
      "\n",
      "Weather distance statistics (km):\n",
      "count    626.000\n",
      "unique   626.000\n",
      "top       32.302\n",
      "freq       1.000\n",
      "Name: weather_distance_km, dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Build nearest neighbor lookup for each PM2.5 hexagon\n",
    "print(\"Building nearest neighbor lookup table...\")\n",
    "print(\"This may take a few minutes...\")\n",
    "\n",
    "nearest_lookup = {}\n",
    "\n",
    "for hex7, hex_info in tqdm(pm25_registry.items(), desc=\"Processing PM2.5 hexagons\"):\n",
    "    lat = hex_info['center_lat']\n",
    "    lon = hex_info['center_lon']\n",
    "    \n",
    "    # Find nearest traffic hexagon\n",
    "    nearest_traffic, traffic_distance = find_nearest_hexagon(hex7, lat, lon, traffic_registry)\n",
    "    \n",
    "    # Find nearest weather hexagon\n",
    "    nearest_weather, weather_distance = find_nearest_hexagon(hex7, lat, lon, weather_registry)\n",
    "    \n",
    "    nearest_lookup[hex7] = {\n",
    "        'nearest_traffic_hex': nearest_traffic,\n",
    "        'traffic_distance_km': traffic_distance,\n",
    "        'has_local_traffic': traffic_distance == 0,\n",
    "        'nearest_weather_hex': nearest_weather,\n",
    "        'weather_distance_km': weather_distance,\n",
    "        'has_local_weather': weather_distance == 0\n",
    "    }\n",
    "\n",
    "# Convert to DataFrame for analysis\n",
    "nearest_df = pd.DataFrame(nearest_lookup).T.reset_index()\n",
    "nearest_df.rename(columns={'index': 'hex7_id'}, inplace=True)\n",
    "\n",
    "print(\"\\nNearest neighbor statistics:\")\n",
    "print(f\"Hexagons with local traffic: {nearest_df['has_local_traffic'].sum()}\")\n",
    "print(f\"Hexagons with local weather: {nearest_df['has_local_weather'].sum()}\")\n",
    "print(f\"\\nTraffic distance statistics (km):\")\n",
    "print(nearest_df[nearest_df['traffic_distance_km'] > 0]['traffic_distance_km'].describe())\n",
    "print(f\"\\nWeather distance statistics (km):\")\n",
    "print(nearest_df[nearest_df['weather_distance_km'] > 0]['weather_distance_km'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 5: Create Enriched Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregating PM2.5 data to resolution 7...\n",
      "Created 9,457,112 hourly PM2.5 records for 634 hexagons\n"
     ]
    }
   ],
   "source": [
    "# Aggregate PM2.5 data to resolution 7\n",
    "print(\"Aggregating PM2.5 data to resolution 7...\")\n",
    "\n",
    "df_pm25['hex7_id'] = df_pm25['h3_index_res8'].apply(\n",
    "    lambda x: h3.cell_to_parent(x, H3_RESOLUTION) if pd.notna(x) else None\n",
    ")\n",
    "\n",
    "# Aggregate to hourly at hex7\n",
    "df_pm25['timestamp'] = pd.to_datetime(df_pm25['timestamp'])\n",
    "df_pm25['hour'] = df_pm25['timestamp'].dt.floor('H')\n",
    "\n",
    "pm25_hourly = df_pm25.groupby(['hex7_id', 'hour']).agg({\n",
    "    'pm25_ugm3_mean': 'mean',\n",
    "    'h3_lat_res8': 'mean',\n",
    "    'h3_lon_res8': 'mean'\n",
    "}).reset_index()\n",
    "\n",
    "pm25_hourly.rename(columns={\n",
    "    'hour': 'timestamp',\n",
    "    'h3_lat_res8': 'lat',\n",
    "    'h3_lon_res8': 'lon'\n",
    "}, inplace=True)\n",
    "\n",
    "print(f\"Created {len(pm25_hourly):,} hourly PM2.5 records for {pm25_hourly['hex7_id'].nunique()} hexagons\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Aggregating traffic data to resolution 7...\n",
      "Created 8,677,621 hourly traffic records for 1017 hexagons\n"
     ]
    }
   ],
   "source": [
    "# Prepare traffic data aggregation\n",
    "print(\"\\nAggregating traffic data to resolution 7...\")\n",
    "\n",
    "df_traffic['hex7_id'] = df_traffic['h3_index_res8'].apply(\n",
    "    lambda x: h3.cell_to_parent(x, H3_RESOLUTION) if pd.notna(x) else None\n",
    ")\n",
    "\n",
    "df_traffic['timestamp'] = pd.to_datetime(df_traffic['timestamp'])\n",
    "df_traffic['hour'] = df_traffic['timestamp'].dt.floor('H')\n",
    "\n",
    "traffic_hourly = df_traffic.groupby(['hex7_id', 'hour']).agg({\n",
    "    'avg_traffic_volume': 'mean'\n",
    "}).reset_index()\n",
    "\n",
    "traffic_hourly.rename(columns={'hour': 'timestamp'}, inplace=True)\n",
    "\n",
    "print(f\"Created {len(traffic_hourly):,} hourly traffic records for {traffic_hourly['hex7_id'].nunique()} hexagons\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Aggregating weather data to resolution 7...\n",
      "Created 3,578,760 hourly weather records for 536 hexagons\n"
     ]
    }
   ],
   "source": [
    "# Prepare weather data aggregation\n",
    "print(\"\\nAggregating weather data to resolution 7...\")\n",
    "\n",
    "df_weather['hex7_id'] = df_weather['h3_index_res8'].apply(\n",
    "    lambda x: h3.cell_to_parent(x, H3_RESOLUTION) if pd.notna(x) else None\n",
    ")\n",
    "\n",
    "df_weather['timestamp'] = pd.to_datetime(df_weather['timestamp'])\n",
    "df_weather['hour'] = df_weather['timestamp'].dt.floor('H')\n",
    "\n",
    "weather_hourly = df_weather.groupby(['hex7_id', 'hour']).agg({\n",
    "    'temperature_c_mean': 'mean',\n",
    "    'humidity_pct_mean': 'mean',\n",
    "    'precipitation_mm_mean': 'mean'\n",
    "}).reset_index()\n",
    "\n",
    "weather_hourly.rename(columns={'hour': 'timestamp'}, inplace=True)\n",
    "\n",
    "print(f\"Created {len(weather_hourly):,} hourly weather records for {weather_hourly['hex7_id'].nunique()} hexagons\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating enriched dataset...\n",
      "Creating lookup dictionaries for faster processing...\n",
      "Created lookups: 8,677,621 traffic, 3,578,760 weather entries\n",
      "Processing FULL dataset...\n",
      "\n",
      "Processing data with parallel enrichment...\n",
      "Total records to process: 9,457,112\n",
      "Using parallel processing with -1 jobs\n",
      "============================================================\n",
      "\n",
      "[PARALLEL] PROCESSING TRAFFIC AND WEATHER ENRICHMENT\n",
      "------------------------------------------------------------\n",
      "Processing records...\n",
      "This may take several minutes for the full dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bccf80ec3094d2b8ce94dae061b8f6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Enriching data:   0%|          | 0/9457112 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Enrichment complete!\n",
      "\n",
      "============================================================\n",
      "✓ ENRICHMENT FINISHED: Created dataset with 9,457,112 records\n",
      "============================================================\n",
      "\n",
      "Saving enriched data...\n",
      "Saving as Parquet to ml_data/pm25_enriched_hourly.parquet...\n",
      "✓ Parquet saved! File size: 31.9 MB\n",
      "\n",
      "Saving as CSV to ml_data/pm25_enriched_hourly.csv...\n",
      "WARNING: Saving 9M+ records to CSV will take several minutes...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64e549452951461aadfd6c4b4ad0a3ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving CSV chunks:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Check if we already have saved enriched data\n",
    "saved_file_csv = 'ml_data/pm25_enriched_hourly.csv'\n",
    "saved_file_parquet = 'ml_data/pm25_enriched_hourly.parquet'\n",
    "\n",
    "# Check for either CSV or Parquet\n",
    "if os.path.exists(saved_file_parquet):\n",
    "    print(f\"Found existing enriched data file: {saved_file_parquet}\")\n",
    "    print(\"Loading saved data...\")\n",
    "    enriched_data = pd.read_parquet(saved_file_parquet)\n",
    "    print(f\"Loaded {len(enriched_data):,} records from saved file\")\n",
    "    print(\"Skipping enrichment process. Delete the file if you want to reprocess.\")\n",
    "elif os.path.exists(saved_file_csv):\n",
    "    print(f\"Found existing enriched data file: {saved_file_csv}\")\n",
    "    print(\"Loading saved data...\")\n",
    "    enriched_data = pd.read_csv(saved_file_csv)\n",
    "    print(f\"Loaded {len(enriched_data):,} records from saved file\")\n",
    "    print(\"Skipping enrichment process. Delete the file if you want to reprocess.\")\n",
    "else:\n",
    "    # Merge data for PM2.5 hexagons with enrichment\n",
    "    print(\"\\nCreating enriched dataset...\")\n",
    "    \n",
    "    # Start with PM2.5 data\n",
    "    enriched_data = pm25_hourly.copy()\n",
    "    \n",
    "    # Add nearest neighbor information\n",
    "    enriched_data = enriched_data.merge(nearest_df, on='hex7_id', how='left')\n",
    "    \n",
    "    # Create global lookups for faster access\n",
    "    print(\"Creating lookup dictionaries for faster processing...\")\n",
    "    \n",
    "    # Create traffic lookup: (hex_id, timestamp) -> data\n",
    "    traffic_lookup = {}\n",
    "    for _, row in traffic_hourly.iterrows():\n",
    "        key = (row['hex7_id'], row['timestamp'])\n",
    "        traffic_lookup[key] = row['avg_traffic_volume']\n",
    "    \n",
    "    # Create weather lookup: (hex_id, timestamp) -> data\n",
    "    weather_lookup = {}\n",
    "    for _, row in weather_hourly.iterrows():\n",
    "        key = (row['hex7_id'], row['timestamp'])\n",
    "        weather_lookup[key] = {\n",
    "            'temperature_c_mean': row['temperature_c_mean'],\n",
    "            'humidity_pct_mean': row['humidity_pct_mean'],\n",
    "            'precipitation_mm_mean': row['precipitation_mm_mean']\n",
    "        }\n",
    "    \n",
    "    print(f\"Created lookups: {len(traffic_lookup):,} traffic, {len(weather_lookup):,} weather entries\")\n",
    "    \n",
    "    # Function to process a single row\n",
    "    def process_row(row_data):\n",
    "        \"\"\"Process a single row for both traffic and weather enrichment\"\"\"\n",
    "        idx, row = row_data\n",
    "        \n",
    "        # Process traffic\n",
    "        if row['has_local_traffic']:\n",
    "            source_hex = row['hex7_id']\n",
    "        else:\n",
    "            source_hex = row['nearest_traffic_hex']\n",
    "        \n",
    "        traffic_result = {}\n",
    "        if pd.notna(source_hex):\n",
    "            key = (source_hex, row['timestamp'])\n",
    "            if key in traffic_lookup:\n",
    "                traffic_result['avg_traffic_volume'] = traffic_lookup[key]\n",
    "                traffic_result['traffic_source'] = 'local' if row['has_local_traffic'] else 'nearest'\n",
    "            else:\n",
    "                traffic_result['avg_traffic_volume'] = np.nan\n",
    "                traffic_result['traffic_source'] = 'missing'\n",
    "        else:\n",
    "            traffic_result['avg_traffic_volume'] = np.nan\n",
    "            traffic_result['traffic_source'] = np.nan\n",
    "        \n",
    "        # Process weather\n",
    "        if row['has_local_weather']:\n",
    "            source_hex = row['hex7_id']\n",
    "        else:\n",
    "            source_hex = row['nearest_weather_hex']\n",
    "        \n",
    "        weather_result = {}\n",
    "        if pd.notna(source_hex):\n",
    "            key = (source_hex, row['timestamp'])\n",
    "            if key in weather_lookup:\n",
    "                weather_data = weather_lookup[key]\n",
    "                weather_result.update(weather_data)\n",
    "                weather_result['weather_source'] = 'local' if row['has_local_weather'] else 'nearest'\n",
    "            else:\n",
    "                weather_result['temperature_c_mean'] = np.nan\n",
    "                weather_result['humidity_pct_mean'] = np.nan\n",
    "                weather_result['precipitation_mm_mean'] = np.nan\n",
    "                weather_result['weather_source'] = 'missing'\n",
    "        else:\n",
    "            weather_result['temperature_c_mean'] = np.nan\n",
    "            weather_result['humidity_pct_mean'] = np.nan\n",
    "            weather_result['precipitation_mm_mean'] = np.nan\n",
    "            weather_result['weather_source'] = np.nan\n",
    "        \n",
    "        # Combine results\n",
    "        combined = {**traffic_result, **weather_result}\n",
    "        return idx, combined\n",
    "    \n",
    "    # Process full dataset or sample\n",
    "    USE_SAMPLE = False  # Process FULL dataset\n",
    "    SAMPLE_SIZE = 10000\n",
    "    \n",
    "    if USE_SAMPLE:\n",
    "        print(f\"Using sample of {SAMPLE_SIZE:,} records for faster processing...\")\n",
    "        enriched_data = enriched_data.head(SAMPLE_SIZE)\n",
    "    else:\n",
    "        print(\"Processing FULL dataset...\")\n",
    "    \n",
    "    print(\"\\nProcessing data with parallel enrichment...\")\n",
    "    total_records = len(enriched_data)\n",
    "    print(f\"Total records to process: {total_records:,}\")\n",
    "    \n",
    "    # Determine number of jobs\n",
    "    n_jobs = min(8, -1)  # Use up to 8 cores or all available\n",
    "    print(f\"Using parallel processing with {n_jobs} jobs\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(\"\\n[PARALLEL] PROCESSING TRAFFIC AND WEATHER ENRICHMENT\")\n",
    "    print(\"-\"*60)\n",
    "    \n",
    "    # Prepare data for processing\n",
    "    row_data = list(enriched_data.iterrows())\n",
    "    \n",
    "    # Process in parallel with progress bar\n",
    "    print(\"Processing records...\")\n",
    "    print(\"This may take several minutes for the full dataset...\")\n",
    "    \n",
    "    results = Parallel(n_jobs=n_jobs, backend='threading')(\n",
    "        delayed(process_row)(row) \n",
    "        for row in tqdm(row_data, desc=\"Enriching data\")\n",
    "    )\n",
    "    \n",
    "    # Sort results by index to maintain order\n",
    "    results.sort(key=lambda x: x[0])\n",
    "    \n",
    "    # Extract enrichment data\n",
    "    enrichment_data = [result[1] for result in results]\n",
    "    enrichment_df = pd.DataFrame(enrichment_data)\n",
    "    \n",
    "    # Add enrichments to the main dataframe\n",
    "    enriched_data = pd.concat([enriched_data, enrichment_df], axis=1)\n",
    "    \n",
    "    print(f\"\\n✓ Enrichment complete!\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"✓ ENRICHMENT FINISHED: Created dataset with {len(enriched_data):,} records\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Create ml_data directory if it doesn't exist\n",
    "    os.makedirs('ml_data', exist_ok=True)\n",
    "    \n",
    "    # Save as both Parquet (fast) and CSV (if needed)\n",
    "    print(f\"\\nSaving enriched data...\")\n",
    "    \n",
    "    # Save as Parquet first (much faster and smaller)\n",
    "    print(f\"Saving as Parquet to {saved_file_parquet}...\")\n",
    "    enriched_data.to_parquet(saved_file_parquet, index=False, compression='snappy')\n",
    "    parquet_size_mb = os.path.getsize(saved_file_parquet) / 1024 / 1024\n",
    "    print(f\"✓ Parquet saved! File size: {parquet_size_mb:.1f} MB\")\n",
    "    \n",
    "    # Optionally save as CSV (this will be slow for large datasets)\n",
    "    SAVE_CSV = True  # Set to True if you really need CSV format\n",
    "    \n",
    "    if SAVE_CSV:\n",
    "        print(f\"\\nSaving as CSV to {saved_file_csv}...\")\n",
    "        print(\"WARNING: Saving 9M+ records to CSV will take several minutes...\")\n",
    "        \n",
    "        # Save in chunks for better performance and progress tracking\n",
    "        chunk_size = 500000  # Save 500k rows at a time\n",
    "        n_chunks = (len(enriched_data) + chunk_size - 1) // chunk_size\n",
    "        \n",
    "        for i in tqdm(range(n_chunks), desc=\"Saving CSV chunks\"):\n",
    "            start_idx = i * chunk_size\n",
    "            end_idx = min((i + 1) * chunk_size, len(enriched_data))\n",
    "            chunk = enriched_data.iloc[start_idx:end_idx]\n",
    "            \n",
    "            # Write header only for first chunk\n",
    "            mode = 'w' if i == 0 else 'a'\n",
    "            header = i == 0\n",
    "            \n",
    "            chunk.to_csv(saved_file_csv, mode=mode, header=header, index=False)\n",
    "        \n",
    "        csv_size_mb = os.path.getsize(saved_file_csv) / 1024 / 1024\n",
    "        print(f\"✓ CSV saved! File size: {csv_size_mb:.1f} MB\")\n",
    "    else:\n",
    "        print(\"\\nSkipping CSV save (set SAVE_CSV=True if needed)\")\n",
    "        print(\"Parquet format is recommended for large datasets (faster & smaller)\")\n",
    "    \n",
    "    # Also save lookup table to ml_data folder\n",
    "    if 'nearest_lookup' in locals():\n",
    "        lookup_file = 'ml_data/hexagon_lookup_table.json'\n",
    "        with open(lookup_file, 'w') as f:\n",
    "            json.dump(nearest_lookup, f, indent=2)\n",
    "        print(f\"✓ Saved hexagon lookup table to {lookup_file}\")\n",
    "    \n",
    "    # Final summary\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"PROCESSING COMPLETE!\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"  Total records processed: {total_records:,}\")\n",
    "    print(f\"  Output file: {saved_file_parquet}\")\n",
    "    print(f\"  File size: {parquet_size_mb:.1f} MB\")\n",
    "    print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Adding temporal features...\")\n",
    "\n",
    "enriched_data['hour'] = enriched_data['timestamp'].dt.hour\n",
    "enriched_data['day_of_week'] = enriched_data['timestamp'].dt.dayofweek\n",
    "enriched_data['month'] = enriched_data['timestamp'].dt.month\n",
    "enriched_data['is_weekend'] = (enriched_data['day_of_week'] >= 5).astype(int)\n",
    "\n",
    "# Cyclical encoding\n",
    "enriched_data['hour_sin'] = np.sin(2 * np.pi * enriched_data['hour'] / 24)\n",
    "enriched_data['hour_cos'] = np.cos(2 * np.pi * enriched_data['hour'] / 24)\n",
    "enriched_data['dow_sin'] = np.sin(2 * np.pi * enriched_data['day_of_week'] / 7)\n",
    "enriched_data['dow_cos'] = np.cos(2 * np.pi * enriched_data['day_of_week'] / 7)\n",
    "enriched_data['month_sin'] = np.sin(2 * np.pi * enriched_data['month'] / 12)\n",
    "enriched_data['month_cos'] = np.cos(2 * np.pi * enriched_data['month'] / 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 6: Data Quality Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Data Quality Analysis\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nPM2.5 Coverage:\")\n",
    "print(f\"  Total hexagons: {enriched_data['hex7_id'].nunique()}\")\n",
    "print(f\"  Total records: {len(enriched_data):,}\")\n",
    "print(f\"  Missing PM2.5: {enriched_data['pm25_ugm3_mean'].isna().mean():.1%}\")\n",
    "\n",
    "print(\"\\nTraffic Data Sources:\")\n",
    "if 'traffic_source' in enriched_data.columns:\n",
    "    print(enriched_data['traffic_source'].value_counts())\n",
    "    nearest_traffic = enriched_data[enriched_data['traffic_source']=='nearest']['traffic_distance_km']\n",
    "    if len(nearest_traffic) > 0:\n",
    "        # Use .values to get the actual mean value\n",
    "        print(f\"\\nAverage distance to traffic data: {nearest_traffic.values.mean():.1f} km\")\n",
    "else:\n",
    "    print(\"  Traffic source information not available\")\n",
    "\n",
    "print(\"\\nWeather Data Sources:\")\n",
    "if 'weather_source' in enriched_data.columns:\n",
    "    print(enriched_data['weather_source'].value_counts())\n",
    "    nearest_weather = enriched_data[enriched_data['weather_source']=='nearest']['weather_distance_km']\n",
    "    if len(nearest_weather) > 0:\n",
    "        # Use .values to get the actual mean value\n",
    "        print(f\"\\nAverage distance to weather data: {nearest_weather.values.mean():.1f} km\")\n",
    "else:\n",
    "    print(\"  Weather source information not available\")\n",
    "\n",
    "print(\"\\nFeature Completeness:\")\n",
    "for col in enriched_data.columns:\n",
    "    if col not in ['hex7_id', 'timestamp', 'nearest_traffic_hex', 'nearest_weather_hex']:\n",
    "        missing_pct = enriched_data[col].isna().mean() * 100\n",
    "        if missing_pct > 0:\n",
    "            print(f\"  {col}: {100-missing_pct:.1f}% complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 7: Save Enriched Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "print(\"Saved datasets in ml_data folder:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Check Parquet file\n",
    "parquet_file = 'ml_data/pm25_enriched_hourly.parquet'\n",
    "if os.path.exists(parquet_file):\n",
    "    size_mb = os.path.getsize(parquet_file) / 1024 / 1024\n",
    "    print(f\"✓ Parquet file: {parquet_file}\")\n",
    "    print(f\"  Size: {size_mb:.1f} MB\")\n",
    "    # Quick load to check shape\n",
    "    temp_df = pd.read_parquet(parquet_file)\n",
    "    print(f\"  Records: {len(temp_df):,}\")\n",
    "    print(f\"  Columns: {len(temp_df.columns)}\")\n",
    "    del temp_df  # Free memory\n",
    "\n",
    "# Check CSV file\n",
    "csv_file = 'ml_data/pm25_enriched_hourly.csv'\n",
    "if os.path.exists(csv_file):\n",
    "    size_mb = os.path.getsize(csv_file) / 1024 / 1024\n",
    "    print(f\"\\n✓ CSV file: {csv_file}\")\n",
    "    print(f\"  Size: {size_mb:.1f} MB\")\n",
    "\n",
    "# Check lookup table\n",
    "lookup_file = 'ml_data/hexagon_lookup_table.json'\n",
    "if os.path.exists(lookup_file):\n",
    "    size_kb = os.path.getsize(lookup_file) / 1024\n",
    "    print(f\"\\n✓ Lookup table: {lookup_file}\")\n",
    "    print(f\"  Size: {size_kb:.1f} KB\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Use pd.read_parquet('ml_data/pm25_enriched_hourly.parquet') to load the data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 8: Query Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PM25QueryInterface:\n",
    "    def __init__(self, enriched_data, pm25_registry):\n",
    "        self.data = enriched_data\n",
    "        self.registry = pm25_registry\n",
    "        self.hex_locations = {hex_id: (info['center_lat'], info['center_lon']) \n",
    "                              for hex_id, info in pm25_registry.items()}\n",
    "    \n",
    "    def find_nearest_pm25_hexagon(self, query_lat, query_lon):\n",
    "        \"\"\"Find the nearest PM2.5 hexagon to a query location\"\"\"\n",
    "        min_distance = float('inf')\n",
    "        nearest_hex = None\n",
    "        \n",
    "        for hex_id, (lat, lon) in self.hex_locations.items():\n",
    "            distance = haversine_distance(query_lat, query_lon, lat, lon)\n",
    "            if distance < min_distance:\n",
    "                min_distance = distance\n",
    "                nearest_hex = hex_id\n",
    "        \n",
    "        return nearest_hex, min_distance\n",
    "    \n",
    "    def get_confidence_score(self, distance_km):\n",
    "        \"\"\"Calculate confidence score based on distance\"\"\"\n",
    "        if distance_km < 5:\n",
    "            return 'high', 0.9\n",
    "        elif distance_km < 20:\n",
    "            return 'medium', 0.7\n",
    "        elif distance_km < 50:\n",
    "            return 'low', 0.5\n",
    "        else:\n",
    "            return 'very_low', 0.3\n",
    "    \n",
    "    def query_location(self, lat, lon, timestamp=None):\n",
    "        \"\"\"Query PM2.5 data for a specific location\"\"\"\n",
    "        \n",
    "        # Find nearest PM2.5 hexagon\n",
    "        nearest_hex, distance_km = self.find_nearest_pm25_hexagon(lat, lon)\n",
    "        \n",
    "        # Get confidence score\n",
    "        confidence_level, confidence_score = self.get_confidence_score(distance_km)\n",
    "        \n",
    "        # Get data for the hexagon\n",
    "        hex_data = self.data[self.data['hex7_id'] == nearest_hex]\n",
    "        \n",
    "        if timestamp:\n",
    "            # Get data for specific timestamp\n",
    "            timestamp = pd.to_datetime(timestamp).floor('H')\n",
    "            hex_data = hex_data[hex_data['timestamp'] == timestamp]\n",
    "        \n",
    "        result = {\n",
    "            'query_location': {'lat': lat, 'lon': lon},\n",
    "            'nearest_hexagon': nearest_hex,\n",
    "            'distance_km': distance_km,\n",
    "            'confidence_level': confidence_level,\n",
    "            'confidence_score': confidence_score,\n",
    "            'data_available': len(hex_data) > 0\n",
    "        }\n",
    "        \n",
    "        if len(hex_data) > 0:\n",
    "            if timestamp:\n",
    "                result['pm25_value'] = hex_data['pm25_ugm3_mean'].iloc[0]\n",
    "                result['traffic_volume'] = hex_data['avg_traffic_volume'].iloc[0]\n",
    "                result['temperature'] = hex_data['temperature_c_mean'].iloc[0]\n",
    "                result['humidity'] = hex_data['humidity_pct_mean'].iloc[0]\n",
    "            else:\n",
    "                result['pm25_mean'] = hex_data['pm25_ugm3_mean'].mean()\n",
    "                result['pm25_std'] = hex_data['pm25_ugm3_mean'].std()\n",
    "                result['data_points'] = len(hex_data)\n",
    "        \n",
    "        return result\n",
    "\n",
    "# Create query interface\n",
    "query_interface = PM25QueryInterface(enriched_data, pm25_registry)\n",
    "print(\"Query interface created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the query interface\n",
    "print(\"Testing query interface...\\n\")\n",
    "\n",
    "# Test locations\n",
    "test_locations = [\n",
    "    (35.6762, 139.6503, \"Tokyo Station\"),\n",
    "    (34.6937, 135.5023, \"Osaka\"),\n",
    "    (43.0642, 141.3469, \"Sapporo\"),\n",
    "    (35.0, 140.0, \"Rural area\")\n",
    "]\n",
    "\n",
    "for lat, lon, name in test_locations:\n",
    "    result = query_interface.query_location(lat, lon)\n",
    "    print(f\"{name} ({lat:.2f}, {lon:.2f}):\")\n",
    "    print(f\"  Nearest PM2.5 sensor: {result['distance_km']:.1f} km away\")\n",
    "    print(f\"  Confidence: {result['confidence_level']} ({result['confidence_score']:.1f})\")\n",
    "    if result['data_available']:\n",
    "        print(f\"  Average PM2.5: {result.get('pm25_mean', 'N/A'):.1f} μg/m³\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 9: Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualization of PM2.5 coverage and data enrichment\n",
    "print(\"Creating coverage map...\")\n",
    "\n",
    "# Create base map\n",
    "center_lat = enriched_data['lat'].mean()\n",
    "center_lon = enriched_data['lon'].mean()\n",
    "\n",
    "coverage_map = folium.Map(\n",
    "    location=[center_lat, center_lon],\n",
    "    zoom_start=6,\n",
    "    tiles='OpenStreetMap'\n",
    ")\n",
    "\n",
    "# Add PM2.5 hexagons\n",
    "for hex_id, info in list(pm25_registry.items())[:500]:  # Limit for performance\n",
    "    # Get hexagon boundary\n",
    "    try:\n",
    "        boundary = h3.cell_to_boundary(hex_id)\n",
    "        \n",
    "        # Check data quality\n",
    "        hex_lookup = nearest_lookup.get(hex_id, {})\n",
    "        has_local_traffic = hex_lookup.get('has_local_traffic', False)\n",
    "        has_local_weather = hex_lookup.get('has_local_weather', False)\n",
    "        \n",
    "        # Color based on data availability\n",
    "        if has_local_traffic and has_local_weather:\n",
    "            color = '#00FF00'  # Green - all local data\n",
    "            fill_opacity = 0.6\n",
    "        elif has_local_traffic or has_local_weather:\n",
    "            color = '#FFFF00'  # Yellow - some local data\n",
    "            fill_opacity = 0.5\n",
    "        else:\n",
    "            color = '#FF0000'  # Red - no local auxiliary data\n",
    "            fill_opacity = 0.4\n",
    "        \n",
    "        folium.Polygon(\n",
    "            locations=boundary,\n",
    "            color=color,\n",
    "            weight=1,\n",
    "            fill=True,\n",
    "            fillColor=color,\n",
    "            fillOpacity=fill_opacity,\n",
    "            popup=f\"\"\"Hexagon: {hex_id}<br>\n",
    "                     Measurements: {info['measurement_count']}<br>\n",
    "                     Local Traffic: {has_local_traffic}<br>\n",
    "                     Local Weather: {has_local_weather}\"\"\",\n",
    "            tooltip=f\"PM2.5 Hexagon\"\n",
    "        ).add_to(coverage_map)\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "# Add legend\n",
    "legend_html = '''\n",
    "<div style=\"position: fixed; \n",
    "            bottom: 50px; right: 50px; width: 250px;\n",
    "            background-color: white; z-index:9999; font-size:14px;\n",
    "            border:2px solid grey; border-radius: 5px; padding: 10px\">\n",
    "<p style=\"margin: 0;\"><b>PM2.5 Hexagon Data Quality</b></p>\n",
    "<hr style=\"margin: 5px 0;\">\n",
    "<p style=\"margin: 5px 0;\"><span style=\"color: #00FF00;\">■</span> Green: Has local traffic & weather</p>\n",
    "<p style=\"margin: 5px 0;\"><span style=\"color: #FFFF00;\">■</span> Yellow: Has some local data</p>\n",
    "<p style=\"margin: 5px 0;\"><span style=\"color: #FF0000;\">■</span> Red: Uses nearest neighbor data</p>\n",
    "</div>\n",
    "'''\n",
    "\n",
    "coverage_map.get_root().html.add_child(folium.Element(legend_html))\n",
    "\n",
    "# Save map\n",
    "coverage_map.save('pm25_enrichment_coverage_map.html')\n",
    "print(\"Map saved as pm25_enrichment_coverage_map.html\")\n",
    "\n",
    "# Display map\n",
    "coverage_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"PM2.5 HEXAGON ENRICHMENT COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nCreated enriched dataset with:\")\n",
    "print(f\"  - {enriched_data['hex7_id'].nunique()} PM2.5 hexagons at H3 resolution 7\")\n",
    "print(f\"  - {len(enriched_data):,} hourly records\")\n",
    "print(f\"  - {len(pm25_with_traffic)} hexagons with local traffic data\")\n",
    "print(f\"  - {len(pm25_with_weather)} hexagons with local weather data\")\n",
    "print(f\"  - Nearest neighbor approximation for remaining hexagons\")\n",
    "print(f\"\\nOutput files:\")\n",
    "print(f\"  - pm25_enriched_hourly.parquet: Enriched dataset\")\n",
    "print(f\"  - hexagon_lookup_table.json: Nearest neighbor lookups\")\n",
    "print(f\"  - pm25_enrichment_coverage_map.html: Coverage visualization\")\n",
    "print(f\"\\nQuery interface ready for location-based PM2.5 lookups\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "smoglens-analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
