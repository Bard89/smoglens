{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PM2.5 Hexagon Data Enrichment - 2023 Complete Dataset\n",
    "\n",
    "This notebook creates a unified dataset for **2023 ONLY** by:\n",
    "1. Identifying all PM2.5 measurement hexagons at H3 resolution 7\n",
    "2. Enriching them with traffic and weather data (local or nearest-neighbor)\n",
    "3. Adding static features like terrain elevation\n",
    "4. Creating a query interface for location-based lookups\n",
    "\n",
    "**Version**: V2 - Complete 2023 data (including Sept-Oct from updated OpenMeteo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 1: Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries loaded successfully\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import h3\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "from tqdm import tqdm\n",
    "import folium\n",
    "from folium import plugins\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "\n",
    "print(\"Libraries loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data path: /Users/vojtech/Code/Bard89/Project-Data/data/processed/\n",
      "H3 Resolution: 7 (approx 5.16 km² per hexagon)\n",
      "Processing Year: 2023\n",
      "Date Range: 2023-01-01 to 2023-12-31\n"
     ]
    }
   ],
   "source": [
    "# Config\n",
    "DATA_PATH = '/Users/vojtech/Code/Bard89/Project-Data/data/processed/'\n",
    "H3_RESOLUTION = 7  # 5.16 km² hexagons\n",
    "\n",
    "# YEAR FILTERING CONFIGURATION\n",
    "PROCESS_YEAR = 2023\n",
    "YEAR_START = f'{PROCESS_YEAR}-01-01'\n",
    "YEAR_END = f'{PROCESS_YEAR}-12-31'\n",
    "\n",
    "print(f\"Data path: {DATA_PATH}\")\n",
    "print(f\"H3 Resolution: {H3_RESOLUTION} (approx 5.16 km² per hexagon)\")\n",
    "print(f\"Processing Year: {PROCESS_YEAR}\")\n",
    "print(f\"Date Range: {YEAR_START} to {YEAR_END}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 2: Create PM2.5 Hexagon Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading PM2.5 air quality data...\n",
      "Original data range: 2023-07-14 16:00:00+00:00 to 2025-07-26 05:00:00+00:00\n",
      "Filtered to 2023: 2023-07-14 16:00:00+00:00 to 2023-12-31 23:00:00+00:00\n",
      "Loaded 2,078,211 PM2.5 records for year 2023\n",
      "PM2.5 missing values: 21.2%\n"
     ]
    }
   ],
   "source": [
    "# Load PM2.5 data and filter for specified year\n",
    "print(\"Loading PM2.5 air quality data...\")\n",
    "df_pm25 = pd.read_csv(f\"{DATA_PATH}jp_openaq_processed_20230101_to_20231231.csv\",\n",
    "                       usecols=['h3_index_res8', 'h3_lat_res8', 'h3_lon_res8', \n",
    "                                'timestamp', 'pm25_ugm3_mean'])\n",
    "\n",
    "# Convert timestamp and filter for year\n",
    "df_pm25['timestamp'] = pd.to_datetime(df_pm25['timestamp'])\n",
    "print(f\"Original data range: {df_pm25['timestamp'].min()} to {df_pm25['timestamp'].max()}\")\n",
    "\n",
    "# Filter for specified year only\n",
    "df_pm25 = df_pm25[(df_pm25['timestamp'] >= YEAR_START) & (df_pm25['timestamp'] <= f'{YEAR_END} 23:59:59')]\n",
    "print(f\"Filtered to {PROCESS_YEAR}: {df_pm25['timestamp'].min()} to {df_pm25['timestamp'].max()}\")\n",
    "\n",
    "print(f\"Loaded {len(df_pm25):,} PM2.5 records for year {PROCESS_YEAR}\")\n",
    "print(f\"PM2.5 missing values: {df_pm25['pm25_ugm3_mean'].isna().mean():.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating PM2.5 hexagon registry at H3 resolution 7...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 638/638 [00:00<00:00, 52779.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created registry with 629 PM2.5 hexagons at resolution 7\n",
      "\n",
      "Top 5 hexagons by measurement count:\n",
      "             hex7_id  center_lat  center_lon  measurement_count\n",
      "466  872f5bc81ffffff      35.675     139.440               6792\n",
      "427  872f5aaccffffff      35.607     139.722               6749\n",
      "464  872f5bc24ffffff      35.465     139.470               6418\n",
      "193  872e61ae1ffffff      34.401     135.304               3492\n",
      "311  872e74906ffffff      36.133     139.612               3458\n"
     ]
    }
   ],
   "source": [
    "# Create PM2.5 hexagon registry at resolution 7\n",
    "print(\"\\nCreating PM2.5 hexagon registry at H3 resolution 7...\")\n",
    "\n",
    "pm25_registry = {}\n",
    "\n",
    "for _, row in tqdm(df_pm25[['h3_index_res8', 'h3_lat_res8', 'h3_lon_res8']].drop_duplicates().iterrows(), \n",
    "                   total=df_pm25['h3_index_res8'].nunique()):\n",
    "    if pd.notna(row['h3_index_res8']):\n",
    "        hex7 = h3.cell_to_parent(row['h3_index_res8'], H3_RESOLUTION)\n",
    "        \n",
    "        if hex7 not in pm25_registry:\n",
    "            pm25_registry[hex7] = {\n",
    "                'hex7_id': hex7,\n",
    "                'center_lat': row['h3_lat_res8'],\n",
    "                'center_lon': row['h3_lon_res8'],\n",
    "                'res8_hexagons': [],\n",
    "                'measurement_count': 0\n",
    "            }\n",
    "        \n",
    "        pm25_registry[hex7]['res8_hexagons'].append(row['h3_index_res8'])\n",
    "\n",
    "# Calculate measurement counts\n",
    "hex7_counts = df_pm25.groupby(\n",
    "    df_pm25['h3_index_res8'].apply(lambda x: h3.cell_to_parent(x, H3_RESOLUTION) if pd.notna(x) else None)\n",
    ")['pm25_ugm3_mean'].count()\n",
    "\n",
    "for hex7, count in hex7_counts.items():\n",
    "    if hex7 in pm25_registry:\n",
    "        pm25_registry[hex7]['measurement_count'] = count\n",
    "\n",
    "print(f\"\\nCreated registry with {len(pm25_registry)} PM2.5 hexagons at resolution 7\")\n",
    "\n",
    "# Convert to DataFrame\n",
    "pm25_hex_df = pd.DataFrame(pm25_registry.values())\n",
    "pm25_hex_df = pm25_hex_df.sort_values('measurement_count', ascending=False)\n",
    "\n",
    "print(\"\\nTop 5 hexagons by measurement count:\")\n",
    "print(pm25_hex_df[['hex7_id', 'center_lat', 'center_lon', 'measurement_count']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 3: Load and Map Auxiliary Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading traffic data...\n",
      "Original traffic data range: 2022-12-31 15:00:00+00:00 to 2023-12-31 14:00:00+00:00\n",
      "Filtered to 2023: 2023-01-01 00:00:00+00:00 to 2023-12-31 14:00:00+00:00\n",
      "Traffic records for 2023: 8,668,679\n",
      "Found 1017 traffic hexagons at resolution 7\n",
      "PM2.5 hexagons with local traffic data: 26 (4.1%)\n"
     ]
    }
   ],
   "source": [
    "# Load traffic data and filter for specified year\n",
    "print(\"Loading traffic data...\")\n",
    "df_traffic = pd.read_csv(f\"{DATA_PATH}jp_jartic_processed_20230101_to_20231231.csv\",\n",
    "                          usecols=['h3_index_res8', 'h3_lat_res8', 'h3_lon_res8',\n",
    "                                   'timestamp', 'avg_traffic_volume'])\n",
    "\n",
    "# Convert timestamp and filter\n",
    "df_traffic['timestamp'] = pd.to_datetime(df_traffic['timestamp'])\n",
    "print(f\"Original traffic data range: {df_traffic['timestamp'].min()} to {df_traffic['timestamp'].max()}\")\n",
    "\n",
    "df_traffic = df_traffic[(df_traffic['timestamp'] >= YEAR_START) & (df_traffic['timestamp'] <= f'{YEAR_END} 23:59:59')]\n",
    "print(f\"Filtered to {PROCESS_YEAR}: {df_traffic['timestamp'].min()} to {df_traffic['timestamp'].max()}\")\n",
    "print(f\"Traffic records for {PROCESS_YEAR}: {len(df_traffic):,}\")\n",
    "\n",
    "# Create traffic hexagon registry at resolution 7\n",
    "traffic_registry = {}\n",
    "for _, row in df_traffic[['h3_index_res8', 'h3_lat_res8', 'h3_lon_res8']].drop_duplicates().iterrows():\n",
    "    if pd.notna(row['h3_index_res8']):\n",
    "        hex7 = h3.cell_to_parent(row['h3_index_res8'], H3_RESOLUTION)\n",
    "        if hex7 not in traffic_registry:\n",
    "            traffic_registry[hex7] = {\n",
    "                'hex7_id': hex7,\n",
    "                'center_lat': row['h3_lat_res8'],\n",
    "                'center_lon': row['h3_lon_res8']\n",
    "            }\n",
    "\n",
    "print(f\"Found {len(traffic_registry)} traffic hexagons at resolution 7\")\n",
    "\n",
    "# Check overlap with PM2.5\n",
    "pm25_with_traffic = set(pm25_registry.keys()) & set(traffic_registry.keys())\n",
    "print(f\"PM2.5 hexagons with local traffic data: {len(pm25_with_traffic)} ({len(pm25_with_traffic)/len(pm25_registry)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weather data...\n",
      "Original weather data range: 2023-01-01 00:00:00+00:00 to 2023-12-31 23:00:00+00:00\n",
      "Filtered to 2023: 2023-01-01 00:00:00+00:00 to 2023-12-31 23:00:00+00:00\n",
      "Weather records for 2023: 4,400,280\n",
      "Months with weather data: [Period('2023-01', 'M'), Period('2023-02', 'M'), Period('2023-03', 'M'), Period('2023-04', 'M'), Period('2023-05', 'M'), Period('2023-06', 'M'), Period('2023-07', 'M'), Period('2023-08', 'M'), Period('2023-09', 'M'), Period('2023-10', 'M'), Period('2023-11', 'M'), Period('2023-12', 'M')]\n",
      "✓ Complete year coverage confirmed!\n",
      "Found 536 weather hexagons at resolution 7\n",
      "PM2.5 hexagons with local weather data: 8 (1.3%)\n"
     ]
    }
   ],
   "source": [
    "# Load weather data and filter for specified year  \n",
    "print(\"Loading weather data...\")\n",
    "# Note: Now using the COMPLETE 2023 data with Sept-Oct included\n",
    "df_weather = pd.read_csv(f\"{DATA_PATH}jp_openmeteo_processed_20230101_to_20231231.csv\",\n",
    "                          usecols=['h3_index_res8', 'h3_lat_res8', 'h3_lon_res8',\n",
    "                                   'timestamp', 'temperature_c_mean', 'humidity_pct_mean',\n",
    "                                   'precipitation_mm_mean'])\n",
    "\n",
    "# Convert timestamp and filter\n",
    "df_weather['timestamp'] = pd.to_datetime(df_weather['timestamp'])\n",
    "print(f\"Original weather data range: {df_weather['timestamp'].min()} to {df_weather['timestamp'].max()}\")\n",
    "\n",
    "df_weather = df_weather[(df_weather['timestamp'] >= YEAR_START) & (df_weather['timestamp'] <= f'{YEAR_END} 23:59:59')]\n",
    "print(f\"Filtered to {PROCESS_YEAR}: {df_weather['timestamp'].min()} to {df_weather['timestamp'].max()}\")\n",
    "print(f\"Weather records for {PROCESS_YEAR}: {len(df_weather):,}\")\n",
    "\n",
    "# Verify we have complete 2023 data\n",
    "unique_months = df_weather['timestamp'].dt.to_period('M').unique()\n",
    "print(f\"Months with weather data: {sorted(unique_months)}\")\n",
    "if len(unique_months) == 12:\n",
    "    print(\"✓ Complete year coverage confirmed!\")\n",
    "else:\n",
    "    print(f\"⚠ Warning: Only {len(unique_months)}/12 months available\")\n",
    "\n",
    "# Create weather hexagon registry at resolution 7\n",
    "weather_registry = {}\n",
    "for _, row in df_weather[['h3_index_res8', 'h3_lat_res8', 'h3_lon_res8']].drop_duplicates().iterrows():\n",
    "    if pd.notna(row['h3_index_res8']):\n",
    "        hex7 = h3.cell_to_parent(row['h3_index_res8'], H3_RESOLUTION)\n",
    "        if hex7 not in weather_registry:\n",
    "            weather_registry[hex7] = {\n",
    "                'hex7_id': hex7,\n",
    "                'center_lat': row['h3_lat_res8'],\n",
    "                'center_lon': row['h3_lon_res8']\n",
    "            }\n",
    "\n",
    "print(f\"Found {len(weather_registry)} weather hexagons at resolution 7\")\n",
    "\n",
    "# Check overlap with PM2.5\n",
    "pm25_with_weather = set(pm25_registry.keys()) & set(weather_registry.keys())\n",
    "print(f\"PM2.5 hexagons with local weather data: {len(pm25_with_weather)} ({len(pm25_with_weather)/len(pm25_registry)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 4: Build Nearest Neighbor Lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def haversine_distance(lat1, lon1, lat2, lon2):\n",
    "    \"\"\"Calculate the great circle distance between two points on Earth\"\"\"\n",
    "    from math import radians, cos, sin, asin, sqrt\n",
    "    \n",
    "    # Convert to radians\n",
    "    lat1, lon1, lat2, lon2 = map(radians, [lat1, lon1, lat2, lon2])\n",
    "    \n",
    "    # Haversine formula\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2\n",
    "    c = 2 * asin(sqrt(a))\n",
    "    r = 6371  # Radius of Earth in kilometers\n",
    "    \n",
    "    return c * r\n",
    "\n",
    "def find_nearest_hexagon(target_hex, target_lat, target_lon, source_registry, max_distance_km=500):\n",
    "    \"\"\"Find nearest hexagon from source registry\"\"\"\n",
    "    min_distance = float('inf')\n",
    "    nearest_hex = None\n",
    "    \n",
    "    for source_hex, source_info in source_registry.items():\n",
    "        if source_hex == target_hex:\n",
    "            # Same hexagon - distance is 0\n",
    "            return source_hex, 0.0\n",
    "        \n",
    "        distance = haversine_distance(target_lat, target_lon, \n",
    "                                       source_info['center_lat'], \n",
    "                                       source_info['center_lon'])\n",
    "        \n",
    "        if distance < min_distance and distance < max_distance_km:\n",
    "            min_distance = distance\n",
    "            nearest_hex = source_hex\n",
    "    \n",
    "    return nearest_hex, min_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building nearest neighbor lookup table...\n",
      "This may take a few minutes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PM2.5 hexagons: 100%|██████████| 629/629 [00:00<00:00, 904.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Nearest neighbor statistics:\n",
      "Hexagons with local traffic: 26\n",
      "Hexagons with local weather: 8\n",
      "\n",
      "Traffic distance statistics (km):\n",
      "count    603.000\n",
      "unique   603.000\n",
      "top        3.899\n",
      "freq       1.000\n",
      "Name: traffic_distance_km, dtype: float64\n",
      "\n",
      "Weather distance statistics (km):\n",
      "count    621.000\n",
      "unique   621.000\n",
      "top       32.302\n",
      "freq       1.000\n",
      "Name: weather_distance_km, dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Build nearest neighbor lookup for each PM2.5 hexagon\n",
    "print(\"Building nearest neighbor lookup table...\")\n",
    "print(\"This may take a few minutes...\")\n",
    "\n",
    "nearest_lookup = {}\n",
    "\n",
    "for hex7, hex_info in tqdm(pm25_registry.items(), desc=\"Processing PM2.5 hexagons\"):\n",
    "    lat = hex_info['center_lat']\n",
    "    lon = hex_info['center_lon']\n",
    "    \n",
    "    # Find nearest traffic hexagon\n",
    "    nearest_traffic, traffic_distance = find_nearest_hexagon(hex7, lat, lon, traffic_registry)\n",
    "    \n",
    "    # Find nearest weather hexagon\n",
    "    nearest_weather, weather_distance = find_nearest_hexagon(hex7, lat, lon, weather_registry)\n",
    "    \n",
    "    nearest_lookup[hex7] = {\n",
    "        'nearest_traffic_hex': nearest_traffic,\n",
    "        'traffic_distance_km': traffic_distance,\n",
    "        'has_local_traffic': traffic_distance == 0,\n",
    "        'nearest_weather_hex': nearest_weather,\n",
    "        'weather_distance_km': weather_distance,\n",
    "        'has_local_weather': weather_distance == 0\n",
    "    }\n",
    "\n",
    "# Convert to DataFrame for analysis\n",
    "nearest_df = pd.DataFrame(nearest_lookup).T.reset_index()\n",
    "nearest_df.rename(columns={'index': 'hex7_id'}, inplace=True)\n",
    "\n",
    "print(\"\\nNearest neighbor statistics:\")\n",
    "print(f\"Hexagons with local traffic: {nearest_df['has_local_traffic'].sum()}\")\n",
    "print(f\"Hexagons with local weather: {nearest_df['has_local_weather'].sum()}\")\n",
    "print(f\"\\nTraffic distance statistics (km):\")\n",
    "print(nearest_df[nearest_df['traffic_distance_km'] > 0]['traffic_distance_km'].describe())\n",
    "print(f\"\\nWeather distance statistics (km):\")\n",
    "print(nearest_df[nearest_df['weather_distance_km'] > 0]['weather_distance_km'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 5: Create Enriched Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregating PM2.5 data to resolution 7...\n",
      "Created 2,049,356 hourly PM2.5 records for 629 hexagons\n"
     ]
    }
   ],
   "source": [
    "# Aggregate PM2.5 data to resolution 7\n",
    "print(\"Aggregating PM2.5 data to resolution 7...\")\n",
    "\n",
    "df_pm25['hex7_id'] = df_pm25['h3_index_res8'].apply(\n",
    "    lambda x: h3.cell_to_parent(x, H3_RESOLUTION) if pd.notna(x) else None\n",
    ")\n",
    "\n",
    "# Aggregate to hourly at hex7\n",
    "df_pm25['timestamp'] = pd.to_datetime(df_pm25['timestamp'])\n",
    "df_pm25['hour'] = df_pm25['timestamp'].dt.floor('H')\n",
    "\n",
    "pm25_hourly = df_pm25.groupby(['hex7_id', 'hour']).agg({\n",
    "    'pm25_ugm3_mean': 'mean',\n",
    "    'h3_lat_res8': 'mean',\n",
    "    'h3_lon_res8': 'mean'\n",
    "}).reset_index()\n",
    "\n",
    "pm25_hourly.rename(columns={\n",
    "    'hour': 'timestamp',\n",
    "    'h3_lat_res8': 'lat',\n",
    "    'h3_lon_res8': 'lon'\n",
    "}, inplace=True)\n",
    "\n",
    "print(f\"Created {len(pm25_hourly):,} hourly PM2.5 records for {pm25_hourly['hex7_id'].nunique()} hexagons\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Aggregating traffic data to resolution 7...\n",
      "Created 8,668,679 hourly traffic records for 1017 hexagons\n"
     ]
    }
   ],
   "source": [
    "# Prepare traffic data aggregation\n",
    "print(\"\\nAggregating traffic data to resolution 7...\")\n",
    "\n",
    "df_traffic['hex7_id'] = df_traffic['h3_index_res8'].apply(\n",
    "    lambda x: h3.cell_to_parent(x, H3_RESOLUTION) if pd.notna(x) else None\n",
    ")\n",
    "\n",
    "df_traffic['timestamp'] = pd.to_datetime(df_traffic['timestamp'])\n",
    "df_traffic['hour'] = df_traffic['timestamp'].dt.floor('H')\n",
    "\n",
    "traffic_hourly = df_traffic.groupby(['hex7_id', 'hour']).agg({\n",
    "    'avg_traffic_volume': 'mean'\n",
    "}).reset_index()\n",
    "\n",
    "traffic_hourly.rename(columns={'hour': 'timestamp'}, inplace=True)\n",
    "\n",
    "print(f\"Created {len(traffic_hourly):,} hourly traffic records for {traffic_hourly['hex7_id'].nunique()} hexagons\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Aggregating weather data to resolution 7...\n",
      "Created 4,400,280 hourly weather records for 536 hexagons\n"
     ]
    }
   ],
   "source": [
    "# Prepare weather data aggregation\n",
    "print(\"\\nAggregating weather data to resolution 7...\")\n",
    "\n",
    "df_weather['hex7_id'] = df_weather['h3_index_res8'].apply(\n",
    "    lambda x: h3.cell_to_parent(x, H3_RESOLUTION) if pd.notna(x) else None\n",
    ")\n",
    "\n",
    "df_weather['timestamp'] = pd.to_datetime(df_weather['timestamp'])\n",
    "df_weather['hour'] = df_weather['timestamp'].dt.floor('H')\n",
    "\n",
    "weather_hourly = df_weather.groupby(['hex7_id', 'hour']).agg({\n",
    "    'temperature_c_mean': 'mean',\n",
    "    'humidity_pct_mean': 'mean',\n",
    "    'precipitation_mm_mean': 'mean'\n",
    "}).reset_index()\n",
    "\n",
    "weather_hourly.rename(columns={'hour': 'timestamp'}, inplace=True)\n",
    "\n",
    "print(f\"Created {len(weather_hourly):,} hourly weather records for {weather_hourly['hex7_id'].nunique()} hexagons\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\nimport numpy as np\nfrom tqdm.notebook import tqdm\nfrom joblib import Parallel, delayed\nfrom datetime import datetime\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Generate timestamp for unique filenames\ntimestamp_str = datetime.now().strftime('%Y%m%d_%H%M%S')\n\n# Output file paths with year and timestamp in filename\nsaved_file_csv = f'../../data/pm25_enriched_hourly_{PROCESS_YEAR}_{timestamp_str}.csv'\nsaved_file_parquet = f'../../data/pm25_enriched_hourly_{PROCESS_YEAR}_{timestamp_str}.parquet'\n\n# Check for existing files with pattern\nimport glob\nexisting_parquet_files = glob.glob(f'../../data/pm25_enriched_hourly_{PROCESS_YEAR}_*.parquet')\nexisting_csv_files = glob.glob(f'../../data/pm25_enriched_hourly_{PROCESS_YEAR}_*.csv')\n\nif existing_parquet_files or existing_csv_files:\n    print(f\"Found existing enriched data files for {PROCESS_YEAR}:\")\n    for f in existing_parquet_files[:3]:\n        print(f\"  - {os.path.basename(f)}\")\n    for f in existing_csv_files[:3]:\n        print(f\"  - {os.path.basename(f)}\")\n    if len(existing_parquet_files) + len(existing_csv_files) > 6:\n        print(f\"  ... and {len(existing_parquet_files) + len(existing_csv_files) - 6} more files\")\n    print(f\"\\nCreating new version with timestamp: {timestamp_str}\")\n\n# Merge data for PM2.5 hexagons with enrichment\nprint(f\"\\nCreating enriched dataset for year {PROCESS_YEAR}...\")\n\n# Start with PM2.5 data\nenriched_data = pm25_hourly.copy()\n\n# Add nearest neighbor information\nenriched_data = enriched_data.merge(nearest_df, on='hex7_id', how='left')\n\n# Create global lookups for faster access\nprint(\"Creating lookup dictionaries for faster processing...\")\n\n# Create traffic lookup: (hex_id, timestamp) -> data\ntraffic_lookup = {}\nfor _, row in traffic_hourly.iterrows():\n    key = (row['hex7_id'], row['timestamp'])\n    traffic_lookup[key] = row['avg_traffic_volume']\n\n# Create weather lookup: (hex_id, timestamp) -> data\nweather_lookup = {}\nfor _, row in weather_hourly.iterrows():\n    key = (row['hex7_id'], row['timestamp'])\n    weather_lookup[key] = {\n        'temperature_c_mean': row['temperature_c_mean'],\n        'humidity_pct_mean': row['humidity_pct_mean'],\n        'precipitation_mm_mean': row['precipitation_mm_mean']\n    }\n\nprint(f\"Created lookups: {len(traffic_lookup):,} traffic, {len(weather_lookup):,} weather entries\")\n\n# Function to process a single row\ndef process_row(row_data):\n    \"\"\"Process a single row for both traffic and weather enrichment\"\"\"\n    idx, row = row_data\n    \n    # Process traffic\n    if row['has_local_traffic']:\n        source_hex = row['hex7_id']\n    else:\n        source_hex = row['nearest_traffic_hex']\n    \n    traffic_result = {}\n    if pd.notna(source_hex):\n        key = (source_hex, row['timestamp'])\n        if key in traffic_lookup:\n            traffic_result['avg_traffic_volume'] = traffic_lookup[key]\n            traffic_result['traffic_source'] = 'local' if row['has_local_traffic'] else 'nearest'\n        else:\n            traffic_result['avg_traffic_volume'] = np.nan\n            traffic_result['traffic_source'] = 'missing'\n    else:\n        traffic_result['avg_traffic_volume'] = np.nan\n        traffic_result['traffic_source'] = np.nan\n    \n    # Process weather\n    if row['has_local_weather']:\n        source_hex = row['hex7_id']\n    else:\n        source_hex = row['nearest_weather_hex']\n    \n    weather_result = {}\n    if pd.notna(source_hex):\n        key = (source_hex, row['timestamp'])\n        if key in weather_lookup:\n            weather_data = weather_lookup[key]\n            weather_result.update(weather_data)\n            weather_result['weather_source'] = 'local' if row['has_local_weather'] else 'nearest'\n        else:\n            weather_result['temperature_c_mean'] = np.nan\n            weather_result['humidity_pct_mean'] = np.nan\n            weather_result['precipitation_mm_mean'] = np.nan\n            weather_result['weather_source'] = 'missing'\n    else:\n        weather_result['temperature_c_mean'] = np.nan\n        weather_result['humidity_pct_mean'] = np.nan\n        weather_result['precipitation_mm_mean'] = np.nan\n        weather_result['weather_source'] = np.nan\n    \n    # Combine results\n    combined = {**traffic_result, **weather_result}\n    return idx, combined\n\n# Process full dataset\nprint(f\"Processing FULL dataset for {PROCESS_YEAR}...\")\n\nprint(\"\\nProcessing data with parallel enrichment...\")\ntotal_records = len(enriched_data)\nprint(f\"Total records to process: {total_records:,}\")\n\n# Determine number of jobs\nn_jobs = min(8, -1)  # Use up to 8 cores or all available\nprint(f\"Using parallel processing with {n_jobs} jobs\")\nprint(\"=\"*60)\n\nprint(f\"\\n[PARALLEL] PROCESSING TRAFFIC AND WEATHER ENRICHMENT FOR {PROCESS_YEAR}\")\nprint(\"-\"*60)\n\n# Prepare data for processing\nrow_data = list(enriched_data.iterrows())\n\n# Process in parallel with progress bar\nprint(\"Processing records...\")\nprint(\"This may take several minutes for the full dataset...\")\n\nresults = Parallel(n_jobs=n_jobs, backend='threading')(\n    delayed(process_row)(row) \n    for row in tqdm(row_data, desc=f\"Enriching {PROCESS_YEAR} data\")\n)\n\n# Sort results by index to maintain order\nresults.sort(key=lambda x: x[0])\n\n# Extract enrichment data\nenrichment_data = [result[1] for result in results]\nenrichment_df = pd.DataFrame(enrichment_data)\n\n# Add enrichments to the main dataframe\nenriched_data = pd.concat([enriched_data, enrichment_df], axis=1)\n\n# Final validation - ensure we only have 2023 data\nenriched_data['timestamp'] = pd.to_datetime(enriched_data['timestamp'])\nyear_check = enriched_data['timestamp'].dt.year.unique()\nprint(f\"\\n✓ Enrichment complete! Years in dataset: {year_check}\")\n\nprint(\"\\n\" + \"=\"*60)\nprint(f\"✓ ENRICHMENT FINISHED: Created {PROCESS_YEAR} dataset with {len(enriched_data):,} records\")\nprint(\"=\"*60)\n\n# Create data directory if it doesn't exist\nos.makedirs('../../data', exist_ok=True)\n\n# Save as both Parquet and CSV (if needed)\nprint(f\"\\nSaving enriched {PROCESS_YEAR} data with timestamp {timestamp_str}...\")\n\nprint(f\"Saving as Parquet to {saved_file_parquet}...\")\nenriched_data.to_parquet(saved_file_parquet, index=False, compression='snappy')\nparquet_size_mb = os.path.getsize(saved_file_parquet) / 1024 / 1024\nprint(f\"✓ Parquet saved! File size: {parquet_size_mb:.1f} MB\")\n\nSAVE_CSV = True  # Set to True if CSV needed\n\nif SAVE_CSV:\n    print(f\"\\nSaving as CSV to {saved_file_csv}...\")\n    print(\"WARNING: Saving large CSV will take several minutes...\")\n    \n    # Save in chunks for better performance and progress tracking\n    chunk_size = 500000  # Save 500k rows at a time\n    n_chunks = (len(enriched_data) + chunk_size - 1) // chunk_size\n    \n    for i in tqdm(range(n_chunks), desc=\"Saving CSV chunks\"):\n        start_idx = i * chunk_size\n        end_idx = min((i + 1) * chunk_size, len(enriched_data))\n        chunk = enriched_data.iloc[start_idx:end_idx]\n        \n        # Write header only for first chunk\n        mode = 'w' if i == 0 else 'a'\n        header = i == 0\n        \n        chunk.to_csv(saved_file_csv, mode=mode, header=header, index=False)\n    \n    csv_size_mb = os.path.getsize(saved_file_csv) / 1024 / 1024\n    print(f\"✓ CSV saved! File size: {csv_size_mb:.1f} MB\")\nelse:\n    print(\"\\nSkipping CSV save (set SAVE_CSV=True if needed)\")\n    print(\"Parquet format is recommended for large datasets (faster & smaller)\")\n\n# Also save lookup table to data folder with timestamp\nif 'nearest_lookup' in locals():\n    lookup_file = f'../../data/hexagon_lookup_table_{PROCESS_YEAR}_{timestamp_str}.json'\n    with open(lookup_file, 'w') as f:\n        json.dump(nearest_lookup, f, indent=2)\n    print(f\"✓ Saved hexagon lookup table to {lookup_file}\")\n\n# Final summary\nprint(\"\\n\" + \"=\"*60)\nprint(f\"PROCESSING COMPLETE FOR {PROCESS_YEAR}!\")\nprint(\"=\"*60)\nprint(f\"  Year processed: {PROCESS_YEAR}\")\nprint(f\"  Total records: {total_records:,}\")\nprint(f\"  Timestamp: {timestamp_str}\")\nprint(f\"  Output files:\")\nprint(f\"    - {saved_file_parquet}\")\nif SAVE_CSV:\n    print(f\"    - {saved_file_csv}\")\nprint(f\"  File size: {parquet_size_mb:.1f} MB (Parquet)\")\nprint(\"=\"*60)"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using enriched data already in memory (2,049,356 records)\n",
      "\n",
      "Adding temporal features...\n",
      "  Extracting hour, day_of_week, month...\n",
      "  Creating cyclical encodings...\n",
      "\n",
      "✓ Temporal features added successfully!\n",
      "  Dataset shape: (2049356, 27)\n",
      "  New features: hour, day_of_week, month, is_weekend, hour_sin, hour_cos, dow_sin, dow_cos, month_sin, month_cos\n",
      "\n",
      "Dataset Summary:\n",
      "----------------------------------------\n",
      "Total records: 2,049,356\n",
      "Total columns: 27\n",
      "Memory usage: 1180.5 MB\n"
     ]
    }
   ],
   "source": [
    "# Add temporal features\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Check if enriched data is already loaded or needs to be loaded from file\n",
    "if 'enriched_data' not in locals() or len(enriched_data) == 0:\n",
    "    print(\"Loading enriched data from saved file...\")\n",
    "    \n",
    "    saved_file_parquet = 'ml_data/pm25_enriched_hourly.parquet'\n",
    "    saved_file_csv = 'ml_data/pm25_enriched_hourly.csv'\n",
    "    \n",
    "    if os.path.exists(saved_file_parquet):\n",
    "        print(f\"Loading from {saved_file_parquet}...\")\n",
    "        enriched_data = pd.read_parquet(saved_file_parquet)\n",
    "        print(f\"✓ Loaded {len(enriched_data):,} records from Parquet\")\n",
    "    elif os.path.exists(saved_file_csv):\n",
    "        print(f\"Loading from {saved_file_csv}...\")\n",
    "        print(\"Note: CSV loading may take a minute for large datasets...\")\n",
    "        enriched_data = pd.read_csv(saved_file_csv)\n",
    "        # Convert timestamp to datetime if it's a string\n",
    "        if 'timestamp' in enriched_data.columns and enriched_data['timestamp'].dtype == 'object':\n",
    "            enriched_data['timestamp'] = pd.to_datetime(enriched_data['timestamp'])\n",
    "        print(f\"✓ Loaded {len(enriched_data):,} records from CSV\")\n",
    "    else:\n",
    "        print(\"ERROR: No saved enriched data found!\")\n",
    "        print(\"Please run cell-17 first to create the enriched dataset.\")\n",
    "        print(\"Expected files:\")\n",
    "        print(f\"  - {saved_file_parquet}\")\n",
    "        print(f\"  - {saved_file_csv}\")\n",
    "else:\n",
    "    print(f\"Using enriched data already in memory ({len(enriched_data):,} records)\")\n",
    "\n",
    "# Now add temporal features\n",
    "if 'enriched_data' in locals() and len(enriched_data) > 0:\n",
    "    \n",
    "    # Check if temporal features already exist\n",
    "    temporal_features = ['hour', 'day_of_week', 'month', 'is_weekend', \n",
    "                        'hour_sin', 'hour_cos', 'dow_sin', 'dow_cos', \n",
    "                        'month_sin', 'month_cos']\n",
    "    \n",
    "    existing_features = [f for f in temporal_features if f in enriched_data.columns]\n",
    "    \n",
    "    if len(existing_features) == len(temporal_features):\n",
    "        print(f\"\\n✓ Temporal features already exist in the dataset\")\n",
    "        print(f\"  Features: {', '.join(existing_features)}\")\n",
    "    else:\n",
    "        print(\"\\nAdding temporal features...\")\n",
    "        \n",
    "        # Ensure timestamp is datetime\n",
    "        if 'timestamp' in enriched_data.columns:\n",
    "            if enriched_data['timestamp'].dtype == 'object':\n",
    "                print(\"  Converting timestamp from string to datetime...\")\n",
    "                enriched_data['timestamp'] = pd.to_datetime(enriched_data['timestamp'])\n",
    "            \n",
    "            # Extract temporal features\n",
    "            print(\"  Extracting hour, day_of_week, month...\")\n",
    "            enriched_data['hour'] = enriched_data['timestamp'].dt.hour\n",
    "            enriched_data['day_of_week'] = enriched_data['timestamp'].dt.dayofweek\n",
    "            enriched_data['month'] = enriched_data['timestamp'].dt.month\n",
    "            enriched_data['is_weekend'] = (enriched_data['day_of_week'] >= 5).astype(int)\n",
    "            \n",
    "            # Cyclical encoding\n",
    "            print(\"  Creating cyclical encodings...\")\n",
    "            enriched_data['hour_sin'] = np.sin(2 * np.pi * enriched_data['hour'] / 24)\n",
    "            enriched_data['hour_cos'] = np.cos(2 * np.pi * enriched_data['hour'] / 24)\n",
    "            enriched_data['dow_sin'] = np.sin(2 * np.pi * enriched_data['day_of_week'] / 7)\n",
    "            enriched_data['dow_cos'] = np.cos(2 * np.pi * enriched_data['day_of_week'] / 7)\n",
    "            enriched_data['month_sin'] = np.sin(2 * np.pi * enriched_data['month'] / 12)\n",
    "            enriched_data['month_cos'] = np.cos(2 * np.pi * enriched_data['month'] / 12)\n",
    "            \n",
    "            print(f\"\\n✓ Temporal features added successfully!\")\n",
    "            print(f\"  Dataset shape: {enriched_data.shape}\")\n",
    "            print(f\"  New features: {', '.join([f for f in temporal_features if f not in existing_features])}\")\n",
    "        else:\n",
    "            print(\"ERROR: No timestamp column found in the dataset!\")\n",
    "    \n",
    "    # Display dataset info\n",
    "    print(\"\\nDataset Summary:\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"Total records: {len(enriched_data):,}\")\n",
    "    print(f\"Total columns: {len(enriched_data.columns)}\")\n",
    "    print(f\"Memory usage: {enriched_data.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
    "    \n",
    "    # Optional: Save the data with temporal features\n",
    "    SAVE_WITH_TEMPORAL = False  # Set to True if you want to save\n",
    "    \n",
    "    if SAVE_WITH_TEMPORAL:\n",
    "        output_file = 'ml_data/pm25_enriched_hourly_with_temporal.parquet'\n",
    "        print(f\"\\nSaving data with temporal features to {output_file}...\")\n",
    "        enriched_data.to_parquet(output_file, index=False, compression='snappy')\n",
    "        file_size_mb = os.path.getsize(output_file) / 1024 / 1024\n",
    "        print(f\"✓ Saved! File size: {file_size_mb:.1f} MB\")\n",
    "else:\n",
    "    print(\"\\nNo data available to add temporal features to.\")\n",
    "    print(\"Please ensure the enriched dataset is created and saved first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 6: Data Quality Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Quality Analysis\n",
      "============================================================\n",
      "\n",
      "PM2.5 Coverage:\n",
      "  Total hexagons: 629\n",
      "  Total records: 2,049,356\n",
      "  Missing PM2.5: 20.7%\n",
      "\n",
      "Traffic Data Sources:\n",
      "traffic_source\n",
      "nearest    1921387\n",
      "local        80447\n",
      "missing      45435\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Average distance to traffic data: 6.1 km\n",
      "\n",
      "Weather Data Sources:\n",
      "weather_source\n",
      "nearest    1967412\n",
      "missing      54417\n",
      "local        27527\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Average distance to weather data: 22.3 km\n",
      "\n",
      "Feature Completeness:\n",
      "  pm25_ugm3_mean: 79.3% complete\n",
      "  avg_traffic_volume: 97.7% complete\n",
      "  traffic_source: 99.9% complete\n",
      "  temperature_c_mean: 97.1% complete\n",
      "  humidity_pct_mean: 97.0% complete\n",
      "  precipitation_mm_mean: 97.0% complete\n"
     ]
    }
   ],
   "source": [
    "print(\"Data Quality Analysis\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nPM2.5 Coverage:\")\n",
    "print(f\"  Total hexagons: {enriched_data['hex7_id'].nunique()}\")\n",
    "print(f\"  Total records: {len(enriched_data):,}\")\n",
    "print(f\"  Missing PM2.5: {enriched_data['pm25_ugm3_mean'].isna().mean():.1%}\")\n",
    "\n",
    "print(\"\\nTraffic Data Sources:\")\n",
    "if 'traffic_source' in enriched_data.columns:\n",
    "    print(enriched_data['traffic_source'].value_counts())\n",
    "    nearest_traffic = enriched_data[enriched_data['traffic_source']=='nearest']['traffic_distance_km']\n",
    "    if len(nearest_traffic) > 0:\n",
    "        # Use .values to get the actual mean value\n",
    "        print(f\"\\nAverage distance to traffic data: {nearest_traffic.values.mean():.1f} km\")\n",
    "else:\n",
    "    print(\"  Traffic source information not available\")\n",
    "\n",
    "print(\"\\nWeather Data Sources:\")\n",
    "if 'weather_source' in enriched_data.columns:\n",
    "    print(enriched_data['weather_source'].value_counts())\n",
    "    nearest_weather = enriched_data[enriched_data['weather_source']=='nearest']['weather_distance_km']\n",
    "    if len(nearest_weather) > 0:\n",
    "        # Use .values to get the actual mean value\n",
    "        print(f\"\\nAverage distance to weather data: {nearest_weather.values.mean():.1f} km\")\n",
    "else:\n",
    "    print(\"  Weather source information not available\")\n",
    "\n",
    "print(\"\\nFeature Completeness:\")\n",
    "for col in enriched_data.columns:\n",
    "    if col not in ['hex7_id', 'timestamp', 'nearest_traffic_hex', 'nearest_weather_hex']:\n",
    "        missing_pct = enriched_data[col].isna().mean() * 100\n",
    "        if missing_pct > 0:\n",
    "            print(f\"  {col}: {100-missing_pct:.1f}% complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query interface created\n"
     ]
    }
   ],
   "source": [
    "class PM25QueryInterface:\n",
    "    def __init__(self, enriched_data, pm25_registry):\n",
    "        self.data = enriched_data\n",
    "        self.registry = pm25_registry\n",
    "        self.hex_locations = {hex_id: (info['center_lat'], info['center_lon']) \n",
    "                              for hex_id, info in pm25_registry.items()}\n",
    "    \n",
    "    def find_nearest_pm25_hexagon(self, query_lat, query_lon):\n",
    "        \"\"\"Find the nearest PM2.5 hexagon to a query location\"\"\"\n",
    "        min_distance = float('inf')\n",
    "        nearest_hex = None\n",
    "        \n",
    "        for hex_id, (lat, lon) in self.hex_locations.items():\n",
    "            distance = haversine_distance(query_lat, query_lon, lat, lon)\n",
    "            if distance < min_distance:\n",
    "                min_distance = distance\n",
    "                nearest_hex = hex_id\n",
    "        \n",
    "        return nearest_hex, min_distance\n",
    "    \n",
    "    def get_confidence_score(self, distance_km):\n",
    "        \"\"\"Calculate confidence score based on distance\"\"\"\n",
    "        if distance_km < 5:\n",
    "            return 'high', 0.9\n",
    "        elif distance_km < 20:\n",
    "            return 'medium', 0.7\n",
    "        elif distance_km < 50:\n",
    "            return 'low', 0.5\n",
    "        else:\n",
    "            return 'very_low', 0.3\n",
    "    \n",
    "    def query_location(self, lat, lon, timestamp=None):\n",
    "        \"\"\"Query PM2.5 data for a specific location\"\"\"\n",
    "        \n",
    "        # Find nearest PM2.5 hexagon\n",
    "        nearest_hex, distance_km = self.find_nearest_pm25_hexagon(lat, lon)\n",
    "        \n",
    "        # Get confidence score\n",
    "        confidence_level, confidence_score = self.get_confidence_score(distance_km)\n",
    "        \n",
    "        # Get data for the hexagon\n",
    "        hex_data = self.data[self.data['hex7_id'] == nearest_hex]\n",
    "        \n",
    "        if timestamp:\n",
    "            # Get data for specific timestamp\n",
    "            timestamp = pd.to_datetime(timestamp).floor('H')\n",
    "            hex_data = hex_data[hex_data['timestamp'] == timestamp]\n",
    "        \n",
    "        result = {\n",
    "            'query_location': {'lat': lat, 'lon': lon},\n",
    "            'nearest_hexagon': nearest_hex,\n",
    "            'distance_km': distance_km,\n",
    "            'confidence_level': confidence_level,\n",
    "            'confidence_score': confidence_score,\n",
    "            'data_available': len(hex_data) > 0\n",
    "        }\n",
    "        \n",
    "        if len(hex_data) > 0:\n",
    "            if timestamp:\n",
    "                result['pm25_value'] = hex_data['pm25_ugm3_mean'].iloc[0]\n",
    "                result['traffic_volume'] = hex_data['avg_traffic_volume'].iloc[0]\n",
    "                result['temperature'] = hex_data['temperature_c_mean'].iloc[0]\n",
    "                result['humidity'] = hex_data['humidity_pct_mean'].iloc[0]\n",
    "            else:\n",
    "                result['pm25_mean'] = hex_data['pm25_ugm3_mean'].mean()\n",
    "                result['pm25_std'] = hex_data['pm25_ugm3_mean'].std()\n",
    "                result['data_points'] = len(hex_data)\n",
    "        \n",
    "        return result\n",
    "\n",
    "# Create query interface\n",
    "query_interface = PM25QueryInterface(enriched_data, pm25_registry)\n",
    "print(\"Query interface created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing query interface...\n",
      "\n",
      "Tokyo Station (35.68, 139.65):\n",
      "  Nearest PM2.5 sensor: 3.3 km away\n",
      "  Confidence: high (0.9)\n",
      "  Average PM2.5: 8.4 μg/m³\n",
      "\n",
      "Osaka (34.69, 135.50):\n",
      "  Nearest PM2.5 sensor: 2.1 km away\n",
      "  Confidence: high (0.9)\n",
      "  Average PM2.5: 7.1 μg/m³\n",
      "\n",
      "Sapporo (43.06, 141.35):\n",
      "  Nearest PM2.5 sensor: 2.3 km away\n",
      "  Confidence: high (0.9)\n",
      "  Average PM2.5: 5.2 μg/m³\n",
      "\n",
      "Rural area (35.00, 140.00):\n",
      "  Nearest PM2.5 sensor: 35.6 km away\n",
      "  Confidence: low (0.5)\n",
      "  Average PM2.5: 9.9 μg/m³\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test the query interface\n",
    "print(\"Testing query interface...\\n\")\n",
    "\n",
    "# Test locations\n",
    "test_locations = [\n",
    "    (35.6762, 139.6503, \"Tokyo Station\"),\n",
    "    (34.6937, 135.5023, \"Osaka\"),\n",
    "    (43.0642, 141.3469, \"Sapporo\"),\n",
    "    (35.0, 140.0, \"Rural area\")\n",
    "]\n",
    "\n",
    "for lat, lon, name in test_locations:\n",
    "    result = query_interface.query_location(lat, lon)\n",
    "    print(f\"{name} ({lat:.2f}, {lon:.2f}):\")\n",
    "    print(f\"  Nearest PM2.5 sensor: {result['distance_km']:.1f} km away\")\n",
    "    print(f\"  Confidence: {result['confidence_level']} ({result['confidence_score']:.1f})\")\n",
    "    if result['data_available']:\n",
    "        print(f\"  Average PM2.5: {result.get('pm25_mean', 'N/A'):.1f} μg/m³\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 9: Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query interface created with 629 hexagon locations\n"
     ]
    }
   ],
   "source": [
    "class PM25QueryInterface:\n",
    "    def __init__(self, enriched_data, pm25_registry=None):\n",
    "        self.data = enriched_data\n",
    "        self.registry = pm25_registry\n",
    "        \n",
    "        # If no registry provided, create from enriched data\n",
    "        if pm25_registry is None:\n",
    "            print(\"Creating hexagon registry from enriched data...\")\n",
    "            self.hex_locations = {}\n",
    "            for hex_id in enriched_data['hex7_id'].unique():\n",
    "                hex_data = enriched_data[enriched_data['hex7_id'] == hex_id].iloc[0]\n",
    "                self.hex_locations[hex_id] = (hex_data['lat'], hex_data['lon'])\n",
    "        else:\n",
    "            self.hex_locations = {hex_id: (info['center_lat'], info['center_lon']) \n",
    "                                  for hex_id, info in pm25_registry.items()}\n",
    "    \n",
    "    def find_nearest_pm25_hexagon(self, query_lat, query_lon):\n",
    "        \"\"\"Find the nearest PM2.5 hexagon to a query location\"\"\"\n",
    "        min_distance = float('inf')\n",
    "        nearest_hex = None\n",
    "        \n",
    "        for hex_id, (lat, lon) in self.hex_locations.items():\n",
    "            distance = haversine_distance(query_lat, query_lon, lat, lon)\n",
    "            if distance < min_distance:\n",
    "                min_distance = distance\n",
    "                nearest_hex = hex_id\n",
    "        \n",
    "        return nearest_hex, min_distance\n",
    "    \n",
    "    def get_confidence_score(self, distance_km):\n",
    "        \"\"\"Calculate confidence score based on distance\"\"\"\n",
    "        if distance_km < 5:\n",
    "            return 'high', 0.9\n",
    "        elif distance_km < 20:\n",
    "            return 'medium', 0.7\n",
    "        elif distance_km < 50:\n",
    "            return 'low', 0.5\n",
    "        else:\n",
    "            return 'very_low', 0.3\n",
    "    \n",
    "    def query_location(self, lat, lon, timestamp=None):\n",
    "        \"\"\"Query PM2.5 data for a specific location\"\"\"\n",
    "        \n",
    "        # Find nearest PM2.5 hexagon\n",
    "        nearest_hex, distance_km = self.find_nearest_pm25_hexagon(lat, lon)\n",
    "        \n",
    "        # Get confidence score\n",
    "        confidence_level, confidence_score = self.get_confidence_score(distance_km)\n",
    "        \n",
    "        # Get data for the hexagon\n",
    "        hex_data = self.data[self.data['hex7_id'] == nearest_hex]\n",
    "        \n",
    "        if timestamp:\n",
    "            # Get data for specific timestamp\n",
    "            timestamp = pd.to_datetime(timestamp).floor('H')\n",
    "            hex_data = hex_data[hex_data['timestamp'] == timestamp]\n",
    "        \n",
    "        result = {\n",
    "            'query_location': {'lat': lat, 'lon': lon},\n",
    "            'nearest_hexagon': nearest_hex,\n",
    "            'distance_km': distance_km,\n",
    "            'confidence_level': confidence_level,\n",
    "            'confidence_score': confidence_score,\n",
    "            'data_available': len(hex_data) > 0\n",
    "        }\n",
    "        \n",
    "        if len(hex_data) > 0:\n",
    "            if timestamp:\n",
    "                result['pm25_value'] = hex_data['pm25_ugm3_mean'].iloc[0]\n",
    "                if 'avg_traffic_volume' in hex_data.columns:\n",
    "                    result['traffic_volume'] = hex_data['avg_traffic_volume'].iloc[0]\n",
    "                if 'temperature_c_mean' in hex_data.columns:\n",
    "                    result['temperature'] = hex_data['temperature_c_mean'].iloc[0]\n",
    "                if 'humidity_pct_mean' in hex_data.columns:\n",
    "                    result['humidity'] = hex_data['humidity_pct_mean'].iloc[0]\n",
    "            else:\n",
    "                result['pm25_mean'] = hex_data['pm25_ugm3_mean'].mean()\n",
    "                result['pm25_std'] = hex_data['pm25_ugm3_mean'].std()\n",
    "                result['data_points'] = len(hex_data)\n",
    "        \n",
    "        return result\n",
    "\n",
    "# Load enriched data if not in memory\n",
    "import os\n",
    "\n",
    "if 'enriched_data' not in locals():\n",
    "    print(\"Loading enriched data...\")\n",
    "    if os.path.exists('ml_data/pm25_enriched_hourly.parquet'):\n",
    "        enriched_data = pd.read_parquet('ml_data/pm25_enriched_hourly.parquet')\n",
    "    elif os.path.exists('ml_data/pm25_enriched_hourly.csv'):\n",
    "        enriched_data = pd.read_csv('ml_data/pm25_enriched_hourly.csv')\n",
    "        enriched_data['timestamp'] = pd.to_datetime(enriched_data['timestamp'])\n",
    "    else:\n",
    "        print(\"ERROR: No enriched data file found!\")\n",
    "        \n",
    "# Create query interface\n",
    "if 'enriched_data' in locals():\n",
    "    # Try to use pm25_registry if it exists, otherwise pass None\n",
    "    registry = pm25_registry if 'pm25_registry' in locals() else None\n",
    "    query_interface = PM25QueryInterface(enriched_data, registry)\n",
    "    print(f\"Query interface created with {len(query_interface.hex_locations)} hexagon locations\")\n",
    "else:\n",
    "    print(\"Cannot create query interface - no enriched data available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PM2.5 HEXAGON ENRICHMENT COMPLETE\n",
      "================================================================================\n",
      "\n",
      "Created enriched dataset with:\n",
      "  - 629 PM2.5 hexagons at H3 resolution 7\n",
      "  - 2,049,356 hourly records\n",
      "  - 26 hexagons with local traffic data\n",
      "  - 8 hexagons with local weather data\n",
      "  - Nearest neighbor approximation for remaining hexagons\n",
      "\n",
      "Output files:\n",
      "  - pm25_enriched_hourly.parquet: Enriched dataset\n",
      "  - hexagon_lookup_table.json: Nearest neighbor lookups\n",
      "  - pm25_enrichment_coverage_map.html: Coverage visualization\n",
      "\n",
      "Query interface ready for location-based PM2.5 lookups\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"PM2.5 HEXAGON ENRICHMENT COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nCreated enriched dataset with:\")\n",
    "print(f\"  - {enriched_data['hex7_id'].nunique()} PM2.5 hexagons at H3 resolution 7\")\n",
    "print(f\"  - {len(enriched_data):,} hourly records\")\n",
    "print(f\"  - {len(pm25_with_traffic)} hexagons with local traffic data\")\n",
    "print(f\"  - {len(pm25_with_weather)} hexagons with local weather data\")\n",
    "print(f\"  - Nearest neighbor approximation for remaining hexagons\")\n",
    "print(f\"\\nOutput files:\")\n",
    "print(f\"  - pm25_enriched_hourly.parquet: Enriched dataset\")\n",
    "print(f\"  - hexagon_lookup_table.json: Nearest neighbor lookups\")\n",
    "print(f\"  - pm25_enrichment_coverage_map.html: Coverage visualization\")\n",
    "print(f\"\\nQuery interface ready for location-based PM2.5 lookups\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "smoglens",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}